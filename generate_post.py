# -*- coding: utf-8 -*-
"""generate_post.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mZXYV91Oj1il0IPBLhEntmJhKmy1rwkF

# **Phase 1: Create the Synthetic LinkedIn Post Dataset**
"""

import random
import pandas as pd

# Define categories
themes = ["Career Advice", "Internship Experience", "Leadership", "Tech Project", "Women in Tech", "First Job", "Learning & Growth"]
tones = ["Humble", "Inspirational", "Grateful", "Visionary", "Confident"]
post_types = ["Milestone", "Reflection", "Announcement", "Story", "Advice"]
audiences = ["Students", "Tech Professionals", "Hiring Managers", "Peers", "Leaders"]
ctas = [
    "Letâ€™s connect and grow together!",
    "Drop your thoughts in the comments!",
    "Tag someone who needs to hear this.",
    "DMs open for anyone curious!",
    "Would love to hear your experiences too."
]

# Function to create a post body
def generate_post_body(theme, tone, audience, post_type):
    return (
        f"As a {random.choice(['student', 'intern', 'young professional'])}, I wanted to share a quick {post_type.lower()} on "
        f"{theme.lower()}. It's been a {tone.lower()} journey connecting with {audience.lower()}, and here's one key insight: "
        f"growth often begins outside your comfort zone."
    )

# Generate synthetic records
records = []
for _ in range(1000):  # You can change this number
    theme = random.choice(themes)
    tone = random.choice(tones)
    post_type = random.choice(post_types)
    audience = random.choice(audiences)
    cta = random.choice(ctas)
    post_body = generate_post_body(theme, tone, audience, post_type)
    records.append({
        "theme": theme,
        "tone": tone,
        "post_type": post_type,
        "audience": audience,
        "post_body": post_body,
        "cta": cta
    })

# Convert to DataFrame
df = pd.DataFrame(records)
df.to_csv("synthetic_linkedin_dataset.csv", index=False)
print("Synthetic dataset generated and saved.")

from google.colab import files
files.download("synthetic_linkedin_dataset.csv")

df.head()

df.tail()

"""# **Phase 2: Model Setup & Fine-Tuning Preparation**

Step 1: Install Required Libraries
"""

!pip install transformers datasets accelerate peft bitsandbytes sentencepiece --quiet

"""Step 2: Load the Model and Tokenizer"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_id = "mistralai/Mistral-7B-Instruct-v0.1"

from huggingface_hub import login
from google.colab import userdata

hf_token = userdata.get('HF_TOKEN')

login(hf_token)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

# Load model in 4-bit for memory-efficient fine-tuning
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto"
)

""" Step 3: Format Dataset for Fine-Tuning"""

import pandas as pd
from datasets import Dataset

df = pd.read_csv("synthetic_linkedin_dataset.csv")

# Combine structured info into prompt
def format_instruction(row):
    return {
        "prompt": (
            f"Write a LinkedIn post.\n"
            f"Theme: {row['theme']}\n"
            f"Tone: {row['tone']}\n"
            f"Post Type: {row['post_type']}\n"
            f"Audience: {row['audience']}\n"
            f"CTA: {row['cta']}\n"
        ),
        "response": row["post_body"]
    }

formatted = df.apply(format_instruction, axis=1).tolist()
ds = Dataset.from_list(formatted)
ds = ds.train_test_split(test_size=0.05)
ds

""" Step 4: Preview a Sample"""

print("Prompt:\n", ds["train"][0]["prompt"])
print("Response:\n", ds["train"][0]["response"])

"""# **Phase 3: Fine-Tuning with LoRA**"""

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

# Prepare model for k-bit training (LoRA-compatible)
model = prepare_model_for_kbit_training(model)

# Define LoRA config
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Apply LoRA to base model
model = get_peft_model(model, lora_config)

# Tokenize dataset
def tokenize(examples):
    # Process each example in the batch
    inputs = [prompt + "\n" + response for prompt, response in zip(examples['prompt'], examples['response'])]
    return tokenizer(
        inputs,
        padding="max_length",
        truncation=True,
        max_length=512
    )

tokenized_ds = ds.map(tokenize, batched=True, remove_columns=["prompt", "response"])

# Training arguments
training_args = TrainingArguments(
    output_dir="./linkedin-llm-lora",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=2,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_steps=50,
    save_total_limit=2,
    report_to="none"
)

# Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# Trainer
trainer = Trainer(
    model=model,
    train_dataset=tokenized_ds["train"],
    args=training_args,
    data_collator=data_collator
)

# Start fine-tuning
trainer.train()

# Save LoRA adapter and tokenizer
model.save_pretrained("linkedin-lora-model")
tokenizer.save_pretrained("linkedin-lora-model")

"""# **Phase 4: Post Generator Inference UI**

Step 1: Load Fine-Tuned Model (LoRA Adapter)
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

model_id = "mistralai/Mistral-7B-Instruct-v0.1"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True
)

base_model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", quantization_config=bnb_config)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Load LoRA fine-tuned adapter
model = PeftModel.from_pretrained(base_model, "linkedin-lora-model")
model.eval()

""" Step 2: Python CLI Inference Function"""

def generate_post(theme, tone, audience, cta, max_tokens=300):
    prompt = (
        f"Write a professional LinkedIn post.\n"
        f"Theme: {theme}\n"
        f"Tone: {tone}\n"
        f"Audience: {audience}\n"
        f"CTA: {cta}\n"
    )

    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)

    with torch.no_grad():
        output = model.generate(
            input_ids=input_ids,
            max_new_tokens=max_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    return decoded.replace(prompt, "").strip()

"""EXAMPLE"""

print(generate_post(
    theme="Women in Tech",
    tone="Inspirational",
    audience="Hiring Managers",
    cta="Tag a woman in tech who inspires you!"
))

"""Step 3: Gradio UI"""

import gradio as gr

def gradio_generate(theme, tone, audience, cta):
    return generate_post(theme, tone, audience, cta)

gr.Interface(
    fn=gradio_generate,
    inputs=[
        gr.Textbox(label="Theme"),
        gr.Textbox(label="Tone"),
        gr.Textbox(label="Audience"),
        gr.Textbox(label="Call-to-Action (CTA)")
    ],
    outputs="text",
    title="LinkedIn Post Generator LLM",
    description="Fine-tuned on synthetic data to generate professional, human-like LinkedIn posts."
).launch()